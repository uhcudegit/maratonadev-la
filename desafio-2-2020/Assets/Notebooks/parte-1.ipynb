{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# MARATÓN BEHIND THE CODE 2020\n",
    "\n",
    "## DESAFÍO 2: PARTE 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En proyectos de ciencia de datos destinados a construir modelos de *aprendizaje automático*, o aprendizaje estadístico, es muy inusual que los datos iniciales ya estén en el formato ideal para la construcción de modelos. Se requieren varios pasos intermedios de preprocesamiento de datos, como la codificación de variables categóricas, normalización de variables numéricas, tratamiento de datos faltantes, etc. La biblioteca **scikit-learn**, una de las bibliotecas de código abierto más populares para *aprendizaje automático* en el mundo, ya tiene varias funciones integradas para realizar las transformaciones de datos más utilizadas. Sin embargo, en un flujo común de un modelo de aprendizaje automático, es necesario aplicar estas transformaciones al menos dos veces: la primera vez para \"entrenar\" el modelo, y luego nuevamente cuando se envían nuevos datos como entrada para ser clasificados por este modelo.\n",
    "\n",
    "Para facilitar el trabajo con este tipo de flujos, scikit-learn también cuenta con una herramienta llamada **Pipeline**, que no es más que una lista ordenada de transformaciones que se deben aplicar a los datos. Para ayudar en el desarrollo y la gestión de todo el ciclo de vida de estas aplicaciones, además del uso de Pipelines, los equipos de científicos de datos pueden utilizar en conjunto **Watson Machine Learning**, que tiene docenas de herramientas para entrenar , gestionar, alojar y evaluar modelos basados ​​en el aprendizaje automático. Además, Watson Machine Learning es capaz de encapsular pipelines y modelos en una API lista para usar e integrarse con otras aplicaciones.\n",
    "\n",
    "Durante el desafío 2, aprenderás como crear un **Pipeline** para un modelo de clasificación y alojarlo como una API con la ayuda de Watson Machine Learning. Una vez alojado, puedes integrar el modelo creado con otras aplicaciones, como asistentes virtuales y más. En este notebook, se presentará un ejemplo funcional de creación de un modelo y un pipeline en scikit-learn (¡que puedes usar como plantilla para tu solución!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trabajando con Pipelines del scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero, realizamos la instalación de scikit-learn versión 0.20.0 en el Kernel de este notebook:\n",
    "!pip install scikit-learn==0.20.0 --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A continuación importaremos varias bibliotecas que se utilizarán:\n",
    "\n",
    "# Biblioteca para trabajar con JSON\n",
    "import json\n",
    "\n",
    "# Biblioteca para realizar solicitudes HTTP\n",
    "import requests\n",
    "\n",
    "# Biblioteca para exploración y análisis de datos\n",
    "import pandas as pd\n",
    "\n",
    "# Biblioteca con métodos numéricos y representaciones matriciales\n",
    "import numpy as np\n",
    "\n",
    "# Biblioteca para construir un modelo basado en la técnica Gradient Boosting\n",
    "import xgboost as xgb\n",
    "\n",
    "# Paquetes scikit-learn para preprocesamiento de datos\n",
    "# \"SimpleImputer\" es una transformación para completar los valores faltantes en conjuntos de datos\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Paquetes de scikit-learn para entrenamiento de modelos y construcción de pipelines\n",
    "# Método para separar el conjunto de datos en muestras de testes y entrenamiento\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Método para crear modelos basados en árboles de decisión\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# Clase para crear una pipeline de machine-learning\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Paquetes scikit-learn para evaluación de modelos\n",
    "# Métodos para la validación cruzada del modelo creado\n",
    "from sklearn.model_selection import KFold, cross_validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importar  un .csv a tu proyecto en IBM Cloud Pak for Data al Kernel de este notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero, importaremos el conjunto de datos proporcionado para el desafío, que ya está incluido en este proyecto.\n",
    "\n",
    "Puedes importar datos desde un archivo .csv directamente al Kernel del portátil como un Pandas DataFrame, que se usa ampliamente para manipular datos en Python.\n",
    "\n",
    "Para realizar la importación, simplemente selecciona la siguiente celda y siga las instrucciones en la imagen a continuación:\n",
    "\n",
    "![alt text](https://i.imgur.com/K1DwL9I.png \"importing-csv-as-df\")\n",
    "\n",
    "Después de seleccionar la opción **\"Insertar en el código\"**, la celda de abajo se llenará con el código necesario para importar y leer los datos en el archivo .csv como un Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "<< inserte el DataFrame Pandas aquí >>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos 16 columnas presentes en el set de datos proporcionado, 15 de las cuales son variables features (datos de entrada) y una de ellas es una variable target (que queremos que nuestro modelo va a predecir).\n",
    "\n",
    "Las variables features son:\n",
    "\n",
    "    Unnamed: 0                          - Esta columna no tiene nombre y debe ser eliminada del dataset\n",
    "    NAME                                - Nombre del estudiante\n",
    "    USER_ID                             - Número de identificación del estudiante\n",
    "    HOURS_DATASCIENCE                   - Número de horas de estudio en Data Science\n",
    "    HOURS_BACKEND                       - Número de horas de estudio en Web (Back-End)\n",
    "    HOURS_FRONTEND                      - Número de horas de estudio en Web (Front-End)\n",
    "    NUM_COURSES_BEGINNER_DATASCIENCE    - Número de cursos de nivel principiante en Data Science completados por el estudiante\n",
    "    NUM_COURSES_BEGINNER_BACKEND        - Número de cursos de nivel principiante en Web (Back-End) completados por el estudiante\n",
    "    NUM_COURSES_BEGINNER_FRONTEND       - Número de cursos de nivel principiante en Web (Front-End) completados por el estudiante\n",
    "    NUM_COURSES_ADVANCED_DATASCIENCE    - Número de cursos de nivel avanzado en Data Science completados por el estudiante\n",
    "    NUM_COURSES_ADVANCED_BACKEND        - Número de cursos de nivel avanzado en Web (Back-End) completados por el estudiante\n",
    "    NUM_COURSES_ADVANCED_FRONTEND       - Número de cursos de nivel avanzado en Web (Front-End) completados por el estudiante\n",
    "    AVG_SCORE_DATASCIENCE               - Promedio acumulado en cursos de Data Science completados por el estudiante\n",
    "    AVG_SCORE_BACKEND                   - Promedio acumulado en cursos de Web (Back-End) completados por el estudiante\n",
    "    AVG_SCORE_FRONTEND                  - Promedio acumulado en cursos de Web (Front-End) completados por el estudiante\n",
    "    \n",
    "La variable target es:\n",
    "\n",
    "    PROFILE                             - Perfil de carrera del estudiante (puede ser uno de 6)\n",
    "    \n",
    "        - beginner_front_end\n",
    "        - advanced_front_end\n",
    "        - beginner_back_end\n",
    "        - advanced_back_end\n",
    "        - beginner_data_science\n",
    "        - advanced_data_science\n",
    "        \n",
    "Con un modelo capaz de clasificar a un alumno en una de estas categorías, podemos recomendar contenidos a los alumnos de forma personalizada según las necesidades de cada alumno."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explorando los datos proporcionados\n",
    "\n",
    "Podemos continuar la exploración de los datos proporcionados con la función ``info()``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualización (visualizations)\n",
    "\n",
    "Para ver el conjunto de datos suministrado, podemos usar las bibliotecas ``matplotlib`` y ``seaborn``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(28, 4))\n",
    "\n",
    "sns.distplot(df_data_1['HOURS_DATASCIENCE'].dropna(), ax=axes[0])\n",
    "sns.distplot(df_data_1['HOURS_BACKEND'].dropna(), ax=axes[1])\n",
    "sns.distplot(df_data_1['HOURS_FRONTEND'].dropna(), ax=axes[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(28, 4))\n",
    "\n",
    "sns.distplot(df_data_1['NUM_COURSES_BEGINNER_DATASCIENCE'].dropna(), ax=axes[0][0])\n",
    "sns.distplot(df_data_1['NUM_COURSES_BEGINNER_BACKEND'].dropna(), ax=axes[0][1])\n",
    "sns.distplot(df_data_1['NUM_COURSES_BEGINNER_FRONTEND'].dropna(), ax=axes[0][2])\n",
    "sns.distplot(df_data_1['NUM_COURSES_ADVANCED_DATASCIENCE'].dropna(), ax=axes[1][0])\n",
    "sns.distplot(df_data_1['NUM_COURSES_ADVANCED_BACKEND'].dropna(), ax=axes[1][1])\n",
    "sns.distplot(df_data_1['NUM_COURSES_ADVANCED_FRONTEND'].dropna(), ax=axes[1][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(28, 4))\n",
    "\n",
    "sns.distplot(df_data_1['AVG_SCORE_DATASCIENCE'].dropna(), ax=axes[0])\n",
    "sns.distplot(df_data_1['AVG_SCORE_BACKEND'].dropna(), ax=axes[1])\n",
    "sns.distplot(df_data_1['AVG_SCORE_FRONTEND'].dropna(), ax=axes[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(28, 4))\n",
    "\n",
    "sns.countplot(ax=axes[0], x='PROFILE', data=df_data_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocesamiento de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el preprocesamiento de los datos, se presentarán en este notebook dos transformaciones básicas, demostrando la construcción de un Pipeline con un modelo funcional. Este Pipeline funcional provisto debe ser mejorado por el participante para que el modelo final alcance la mayor precisión posible, garantizando una mayor puntuación en el desafío. Esta mejora solo se puede realizar en el preprocesamiento de los datos, en la elección de un algoritmo para el entrenamiento de diferentes modelos, o incluso en la alteración del **framework** utilizado (sin embargo, solo se entregará un ejemplo de integración de Watson Machine Learning con *scikit-learn*).\n",
    "\n",
    "La primera transformación (paso en nuestro Pipeline) será la exclusión de la columna \"NOMBRE\" de nuestro conjunto de datos, que además de no ser una variable numérica, tampoco es una variable relacionada con el desempeño de los estudiantes en las disciplinas. Hay funciones listas para usar en *scikit-learn* para realizar esta transformación, sin embargo, nuestro ejemplo demostrará cómo crear una transformación personalizada desde cero en scikit-learn. Si lo desea, el participante puede usar este ejemplo para crear otras transformaciones y agregarlas al Pipeline final :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformación 1: excluir columnas del conjunto de datos\n",
    "\n",
    "Para la creación de una transformación de datos personalizada en scikit-learn, es necesario crear una clase con los métodos ``transform`` y ``fit``. En el método de 'transform', se ejecutará la lógica de nuestra transformación.\n",
    "\n",
    "La siguiente celda muestra el código completo de una transformación ``DropColumns`` para eliminar columnas de un pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "# All sklearn Transforms must have the `transform` and `fit` methods\n",
    "class DropColumns(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Primero copiamos el dataframe de datos de entrada 'X'\n",
    "        data = X.copy()\n",
    "        # Devolvemos un nuevo dataframe de datos sin las columnas no deseadas\n",
    "        return data.drop(labels=self.columns, axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para aplicar esa transformación en un pandas DataFrame pandas, basta instanciar un objeto *DropColumns* y llamar el método transform()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación de instancias de una transformación DropColumns\n",
    "rm_columns = DropColumns(\n",
    "    columns=[\"NAME\", \"Unnamed: 0\"]  # Esta transformación toma como parámetro una lista con los nombres de las columnas no deseadas\n",
    ")\n",
    "\n",
    "print(rm_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver las columnas del conjunto de datos original\n",
    "print(\"Columnas del conjunto de datos original: \\n\")\n",
    "print(df_data_1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar la transformación ``DropColumns`` al conjunto de datos base\n",
    "rm_columns.fit(X=df_data_1)\n",
    "\n",
    "# Reconstruyendo un DataFrame de Pandas con el resultado de la transformación\n",
    "df_data_2 = pd.DataFrame.from_records(\n",
    "    data=rm_columns.transform(\n",
    "        X=df_data_1\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver las columnas del conjunto de datos transformado\n",
    "print(\"Columnas del conjunto de datos después de la transformación ``DropColumns``: \\n\")\n",
    "print(df_data_2.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenga en cuenta que la columna \"NOMBRE\" se ha eliminado y nuestro conjunto de datos ahora solo tiene 14 columnas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformación 2: tratamiento de datos faltantes\n",
    "\n",
    "Para manejar los datos que faltan en nuestro conjunto de datos, ahora usaremos una transformación lista para usar de la biblioteca scikit-learn, llamada **SimpleImputer**.\n",
    "\n",
    "Esta transformación permite varias estrategias para el tratamiento de datos faltantes. La documentación oficial se puede encontrar en: https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html\n",
    "\n",
    "En este ejemplo, simplemente haremos cero todos los valores faltantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un objeto ``SimpleImputer``\n",
    "si = SimpleImputer(\n",
    "    missing_values=np.nan,  # los valores que faltan son del tipo ``np.nan`` (Pandas estándar)\n",
    "    strategy='constant',  # la estrategia elegida es cambiar el valor faltante por una constante\n",
    "    fill_value=0,  # la constante que se usará para completar los valores faltantes es un int64 = 0\n",
    "    verbose=0,\n",
    "    copy=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver los datos faltantes del conjunto de datos antes de la primera transformación (df_data_2)\n",
    "print(\"Valores nulos antes de la transformación SimpleImputer: \\n\\n{}\\n\".format(df_data_2.isnull().sum(axis = 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicamos el SimpleImputer ``si`` al conjunto de datos df_data_2 (resultado de la primera transformación)\n",
    "si.fit(X=df_data_2)\n",
    "\n",
    "# Reconstrucción de un nuevo DataFrame de Pandas con el conjunto imputado (df_data_3)\n",
    "df_data_3 = pd.DataFrame.from_records(\n",
    "    data=si.transform(\n",
    "        X=df_data_2\n",
    "    ),  # el resultado SimpleImputer.transform (<< pandas dataframe >>) es lista lista\n",
    "    columns=df_data_2.columns  # las columnas originales deben conservarse en esta transformación\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver los datos faltantes del conjunto de datos después de la segunda transformación (SimpleImputer) (df_data_3)\n",
    "print(\"Valores nulos en el conjunto de datos después de la transformación SimpleImputer: \\n\\n{}\\n\".format(df_data_3.isnull().sum(axis = 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenga en cuenta que ya no tenemos valores perdidos en nuestro conjunto de datos :)\n",
    "\n",
    "Vale la pena señalar que cambiar los valores perdidos por 0 no siempre es la mejor estrategia. Se anima al participante a estudiar e implementar diferentes estrategias para tratar los valores perdidos para mejorar su modelo y mejorar su puntuación final."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenando un modelo de clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez finalizado el preprocesamiento, ya tenemos el conjunto de datos en el formato necesario para entrenar nuestro modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el ejemplo proporcionado, usaremos todas las columnas, excepto la columna **Profile** como *feautres* (variables de entrada).\n",
    "\n",
    "La variable **Profile** será la variable objetivo del modelo, como se describe en la declaración de desafío."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definición de features del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de las columnas que seran features (Notese que la columna NOMBRE no esta presente)\n",
    "features = [\n",
    "    \"USER_ID\", \"HOURS_DATASCIENCE\", \"HOURS_BACKEND\", \"HOURS_FRONTEND\",\n",
    "    \"NUM_COURSES_BEGINNER_DATASCIENCE\", \"NUM_COURSES_BEGINNER_BACKEND\", \"NUM_COURSES_BEGINNER_FRONTEND\",\n",
    "    \"NUM_COURSES_ADVANCED_DATASCIENCE\", \"NUM_COURSES_ADVANCED_BACKEND\", \"NUM_COURSES_ADVANCED_FRONTEND\",\n",
    "    \"AVG_SCORE_DATASCIENCE\", \"AVG_SCORE_BACKEND\", \"AVG_SCORE_FRONTEND\"\n",
    "]\n",
    "\n",
    "# Definición de variable objetivo\n",
    "target = ['PROFILE']\n",
    "\n",
    "# Preparación de los argumentos para los métodos de la biblioteca ``scikit-learn``\n",
    "X = df_data_3[features]\n",
    "y = df_data_3[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El conjunto de entrada (X):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La variable objetivo (y):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separar el conjunto de datos en un conjunto de entrenamiento y un conjunto de prueba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separaremos el conjunto de datos provisto en dos grupos: uno para entrenar nuestro modelo y otro para probar el resultado a través de una prueba ciega. La separación del conjunto de datos se puede hacer fácilmente con el método *train_test_split ()* de scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separación de datos en conjunto de entrenamiento y conjunto de pruebas\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creando un modelo basado en árboles de decisión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el ejemplo proporcionado, crearemos un clasificador basado en **árboles de decisión**.\n",
    "\n",
    "El primer paso es básicamente crear una instancia de un objeto *DecisionTreeClassifier ()* de la biblioteca scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando el árbol de decisiones con la biblioteca ``scikit-learn``:\n",
    "dtc_model = DecisionTreeClassifier()  # El modelo se creará con los parámetros estándar de la biblioteca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Material teórico sobre árboles de decisión en la documentación oficial de scikit-learn: https://scikit-learn.org/stable/modules/tree.html\n",
    "\n",
    "Una guía para principiantes del mundo del aprendizaje automático: https://developer.ibm.com/es/patterns/use-icp4d-to-build-the-machine-learning-model-for-return-propensity/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejecucion del entrenamiento del árbol de descisión "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento de modelos (llamado método *fit ()* con conjuntos de entrenamiento)\n",
    "dtc_model.fit(\n",
    "    X_train,\n",
    "    y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejecución de predicciones y evaluación del modelo creado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realización de una prueba a ciegas en el modelo creado\n",
    "y_pred = dtc_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Precisión lograda por el árbol de decisiones\n",
    "print(\"Exactitud: {}%\".format(100*round(accuracy_score(y_test, y_pred), 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este notebook demostró cómo trabajar con transformaciones y modelos con la biblioteca scikit-learn. Se recomienda que el participante realice sus experimentos editando el código proporcionado aquí hasta lograr un modelo con alta precisión.\n",
    "\n",
    "Cuando esté satisfecho con su modelo, puede pasar al segundo paso del desafío: encapsular su modelo como una API REST lista para usar con Watson Machine Learning.\n",
    "\n",
    "El notebook para la segunda etapa ya está en este proyecto, simplemente acceda a la pestaña **ASSETS** e inícielo. No olvide apagar primero el Kernel en este portátil para reducir el consumo de su nivel gratuito de IBM Cloud Pak for Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
