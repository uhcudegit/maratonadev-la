{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert project token here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# MARATONA BEHIND THE CODE 2020\n",
    "\n",
    "## DESAFIO 7 - BanCoppel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalando Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install imblearn --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descargar los conjuntos de datos en formato .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --no-check-certificate --content-disposition https://gitlab.com/JoaoPedroPP/datasets/-/raw/master/training_dataset.csv\n",
    "df_training_dataset = pd.read_csv(r'training_dataset.csv')\n",
    "df_training_dataset.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sobre el archivo \"training_dataset.csv\", temos alguna información sobre los usuarios de la app de BanCoppel:\n",
    "\n",
    "**ID**\n",
    "\n",
    "**Genero**\n",
    "\n",
    "**Idade**\n",
    "\n",
    "**Estado_civil**\n",
    "\n",
    "**Trabajo**\n",
    "\n",
    "**Sector**\n",
    "\n",
    "**Telefono**\n",
    "\n",
    "**Anos_usando_internet_banking**\n",
    "\n",
    "**Anos_usando_banca_movil**\n",
    "\n",
    "**Banca_movil_userfriendly**\n",
    "\n",
    "**Frecuencia_internet_banking_mes**\n",
    "\n",
    "**Frecuencia_banca_movil_mes**\n",
    "\n",
    "**Frecuencia_pagamentos_tarjeta_mes**\n",
    "\n",
    "**Frecuencia_tarjeta_virtual_mes**\n",
    "\n",
    "**Frecuencia_saldo_cuenta_mes**\n",
    "\n",
    "**TARGET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training_dataset.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Desglose del desafío: clasificación multiclase\n",
    "\n",
    "Este es un desafio cuyo objetivo de negocio es la segmentación de usuario de aplicativos de BanCopp. Podemos utilizar  dos enfoques: Machine Learning supervisado (clasificación) o no supervisado (clustering). En este desafío será usado la clasificación porque el conjunto de datos ya está disponible con \"labels\", o en otras palabras, ya con la variable objetivo. \n",
    "\n",
    "En la biblioteca scikit-learn tenemos diversos algoritmos para clasificación. El participante es libre para utilizar el framework que desee para completar este desafío.\n",
    "\n",
    "En este notebook será mostrado un ejemplo usando el algoritmo \"Decision Tree\" para clasificar los estudantes en seis diferentes perfiles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ¡Atención!\n",
    "\n",
    "La columna objetivo de este desafío es la columna ``TARGET``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-procesando el dataset antes del entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removiendo todas las lineas que poseen algun valor nulo en determinadas columnas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando el método Pandas **DataFrame.dropna()** usted puede remover todas las lineas nulas del dataset.\n",
    "\n",
    "Docs: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrando los datos nulos del dataset antes de la primera transformación (df)\n",
    "print(\"Valores nulos del df_training_dataset antes de la transformación DropNA: \\n\\n{}\\n\".format(df_training_dataset.isnull().sum(axis = 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicando la función para borrar todas las lines con valore NaN en las siguientes columnas:\n",
    "df_training_dataset = df_training_dataset.dropna(axis='index', how='any', subset=['Genero', 'Años', 'Estado_civil', 'Trabajo', 'Sector', 'Telefono', 'Anos_usando_internet_banking', 'Anos_usando_banca_movil', 'Frecuencia_internet_banking_mes', 'Frecuencia_banca_movil_mes', 'Frecuencia_pagamentos_tarjeta_mes', 'Frecuencia_tarjeta_virtual_mes', 'Frecuencia_saldo_cuenta_mes', 'TARGET'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Mostrando los datos nulos del dataset despues de la primera transformación (df)\n",
    "print(\"Valores nulos del df_training_dataset despues de la transformación DropNA: \\n\\n{}\\n\".format(df_training_dataset.isnull().sum(axis = 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procesando valores NaN con SimpleImputer de sklearn\n",
    "\n",
    "Para los valores NaN, usaremos a substituición por la constante 0 como **ejemplo**.\n",
    "\n",
    "Usted puede escoger la estrategía que crea mejor para tratar los valores nulos :)\n",
    "\n",
    "Docs: https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html?highlight=simpleimputer#sklearn.impute.SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "impute_zeros = SimpleImputer(\n",
    "    missing_values=np.nan,\n",
    "    strategy='constant',\n",
    "    fill_value=0,\n",
    "    verbose=0,\n",
    "    copy=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrando los datos nulos del dataset antes de la segunda transformación (df)\n",
    "print(\"Valores nulos del df_training_dataset antes de la transformación SimpleImputer: \\n\\n{}\\n\".format(df_training_dataset.isnull().sum(axis = 0)))\n",
    "\n",
    "# Aplicando la transformación ``SimpleImputer`` en conjunto de datos base\n",
    "impute_zeros.fit(X=df_training_dataset)\n",
    "\n",
    "# Reconstruyendo un Pandas DataFrame con los resultados\n",
    "df_training_dataset_imputed = pd.DataFrame.from_records(\n",
    "    data=impute_zeros.transform(\n",
    "        X=df_training_dataset\n",
    "    ),\n",
    "    columns=df_training_dataset.columns\n",
    ")\n",
    "\n",
    "# Mostrando los datos nulos del dataset despues de la segunda transformación (df)\n",
    "print(\"Valores nulos del df_training_dataset despues de la transformación SimpleImputer: \\n\\n{}\\n\".format(df_training_dataset_imputed.isnull().sum(axis = 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminando colunas no desadas\n",
    "\n",
    "Vamos  **demonstrar** abajo como usar el método **DataFrame.drop()**.\n",
    "\n",
    "Docs: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training_dataset_imputed.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training_dataset_rmcolumns = df_training_dataset_imputed.drop(columns=['ID', 'Genero', 'Estado_civil', 'Trabajo', 'Sector', 'Telefono'], inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training_dataset_rmcolumns.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ¡Atención!\n",
    "\n",
    "Las columnas removidas anteriores son solo por ejemplo, puede usar las columnas que desee e incluso crear nuevas columnas con datos que crea que son importantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tratamiento de variables categóricas\n",
    "\n",
    "Como mencionado antes, los computadores no son buenos con variables \"categóricas\" (strings).\n",
    "\n",
    "Dado una columna con variable categórica, lo que podemos realizar es la codificación de esa columna en multiples columnas contiendo variables binárias. Este proceso es llamado \"one-hot-encoding\" o \"dummy encoding\". Si usted no esta familiarizado con estos terminos, usted puede buscar mas sobre estos en internet :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tratando variables categóricas con el método Pandas ``get_dummies()''\n",
    "df_training = pd.get_dummies(df_training_dataset_rmcolumns, columns=['Banca_movil_userfriendly'])\n",
    "df_training.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ¡Atención!\n",
    "\n",
    "La columna **TARGET** debe ser mantenida como una string. Usted no necesita procesar/codificar la variable objetivo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenando un clasificador con base en un árbol de decisión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecionando FEATURES y definiendo la variable TARGET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df_training[\n",
    "    [\n",
    "        'Años', 'Anos_usando_internet_banking', 'Anos_usando_banca_movil',\n",
    "       'Frecuencia_internet_banking_mes', 'Frecuencia_banca_movil_mes',\n",
    "       'Frecuencia_pagamentos_tarjeta_mes', 'Frecuencia_tarjeta_virtual_mes',\n",
    "       'Frecuencia_saldo_cuenta_mes', 'Banca_movil_userfriendly_No',\n",
    "       'Banca_movil_userfriendly_Si'\n",
    "    ]\n",
    "]\n",
    "target = df_training['TARGET']  ## No cambie esta variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividiendo el conjunto de datos en conjuntos de entrenamiento y pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.33, random_state=133)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenando un árbol de decisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Método para creacion de modelos basados en arbol de desición\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "dtc = DecisionTreeClassifier(max_depth=15).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Haciendo predicciones en la muestra de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = dtc.predict(X_test)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analizando la calidad del modelo a través de la matriz de confusión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, target_names, title='Confusion matrix', cmap=None, normalize=True):\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.2f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "plot_confusion_matrix(confusion_matrix(y_test, y_pred), ['perfil0', 'perfil1', 'perfil2', 'perfil3', 'perfil4', 'perfil5'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring de los datos necesarios para entregar la solución"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como entrega de su solución, esperamos los resultados clasificados del seguiente dataset llamado \"to_be_scored.csv\":"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download la \"Hoja de evaluación\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!wget --no-check-certificate --content-disposition https://gitlab.com/JoaoPedroPP/datasets/-/raw/master/to_be_scored.csv\n",
    "df_to_be_scored = pd.read_csv(r'to_be_scored.csv')\n",
    "df_to_be_scored.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ¡Atención!\n",
    "\n",
    "El dataframe ``to_be_scored`` en su \"hoja de evaluación\". Note que la columna \"categoria\" no existe en esta muetra, por lo que no puede ser utilizada para entrenar modelos supervisados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_to_be_scored.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# ¡Atención!\n",
    "\n",
    "# Para poder aplicar su modelo y clasificar la hoja de evaluación, usted debe aplicar primero todas las transformaciones de columna que usted aplico en el dataset de entrenamiento.\n",
    "\n",
    "# No remueva o adicione filas en la hoja de evaluación. \n",
    "\n",
    "# No altere el orden de las filas en la hoja de evaluación. \n",
    "\n",
    "# Al final, las 1000 entradas deben estar clasificadas, con los valores calculados en una columna llamada \"target\"\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la celda de abajo, repetimos rapidamente los mismos pasos de pré-procesamiento usados en el ejemplo dado con árbol de decisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - Removiendo filas con valores NaN \n",
    "df_to_be_scored_1 = df_to_be_scored.dropna(axis='index', how='any', subset=['Genero', 'Años', 'Estado_civil', 'Trabajo', 'Sector', 'Telefono', 'Anos_usando_internet_banking', 'Anos_usando_banca_movil', 'Frecuencia_internet_banking_mes', 'Frecuencia_banca_movil_mes', 'Frecuencia_pagamentos_tarjeta_mes', 'Frecuencia_tarjeta_virtual_mes', 'Frecuencia_saldo_cuenta_mes'])\n",
    "\n",
    "# 2 - Reemplazando con ceros los valores faltantes\n",
    "impute_zeros.fit(X=df_to_be_scored_1)\n",
    "df_to_be_scored_2 = pd.DataFrame.from_records(\n",
    "    data=impute_zeros.transform(\n",
    "        X=df_to_be_scored_1\n",
    "    ),\n",
    "    columns=df_to_be_scored_1.columns\n",
    ")\n",
    "\n",
    "# 3 - Remoción de columnas\n",
    "df_to_be_scored_3 = df_to_be_scored_2.drop(columns=['ID', 'Genero', 'Estado_civil', 'Trabajo', 'Sector', 'Telefono'], inplace=False)\n",
    "\n",
    "# 4 - Encoding con \"dummy variables\"\n",
    "df_to_be_scored_4 = pd.get_dummies(df_to_be_scored_3, columns=['Banca_movil_userfriendly'])\n",
    "\n",
    "df_to_be_scored_4.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "Puede verificar abajo que las columnas de la hoja de evaluación son identicas a las usadas para entrenar el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training[\n",
    "    [\n",
    "        'Años', 'Anos_usando_internet_banking', 'Anos_usando_banca_movil',\n",
    "       'Frecuencia_internet_banking_mes', 'Frecuencia_banca_movil_mes',\n",
    "       'Frecuencia_pagamentos_tarjeta_mes', 'Frecuencia_tarjeta_virtual_mes',\n",
    "       'Frecuencia_saldo_cuenta_mes', 'Banca_movil_userfriendly_No',\n",
    "       'Banca_movil_userfriendly_Si'\n",
    "    ]\n",
    "].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_be_scored_4.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atención\n",
    "\n",
    "Para todas las columnas que no existiren en \"df_to_be_scored\", usted puede usar la tecnica de abajo para adicionarla:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_be_scored_4['Banca_movil_userfriendly_No'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_pred = dtc.predict(df_to_be_scored_4)\n",
    "df_to_be_scored_4['TARGET'] = y_pred\n",
    "df_to_be_scored_4.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardando la hoja de evaluacion como un archivo .csv para ser enviado\n",
    "**SI YA TIENE UN DATA ASSET CON EL NOMBRE results.csv EN ESTE PROYECTO O EN OTRO PROYECTO DE WATSON STUDIO DEBE BORRAR EL ARCHIVO ANTES DE CORRER LA SIGUIENTE CELDA O TENDRA EL ERROR: _RuntimeError: File 'results.csv' already exists in storage._**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.save_data(file_name=\"results.csv\", data=df_to_be_scored_4.to_csv(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATENCIÓN\n",
    "\n",
    "# La ejecución de la celda anterior creará un nuevo \"data asset\" en su proyecto de Watson Studio. Deberá descargar este archivo junto con este cuaderno y crear un archivo zip con results.csv y notebook.ipynb para enviarlo. (los archivos deben tener este nombre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## ¡Felicitaciones!\n",
    "Si ya está satisfecho con su solución, vaya a la página siguiente y envíe los archivos necesarios para su envío.\n",
    "\n",
    "# https://bancoppel.maratona.dev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
