{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# MARATONA BEHIND THE CODE 2020\n",
    "\n",
    "## DESAFIO 4 - CompuSoluciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este desafío, CompuSoluciones usará herramientas de IBM como Watson Studio (o Cloud Pack for Data) y Watson Machine Learning para construir un modelo de Machine Learning natural capaz de predecir la probabilidad de cumplimiento de pago.\n",
    "\n",
    "La idea esencial del Desafío 4 es crear un modelo basado en machine learning capaz de identificar el comportamiento financiero del asociado de negocio, permitiendo una probabilidad de cumplimiento o incumplimiento del crédito."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalación de Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn==0.20.3 --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si desea utilizar la biblioteca ** xgboost **, instale la versión 0.71.\n",
    "#!pip install xgboost==0.71 --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from datetime import datetime\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descargando el dataset csv desde Github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --no-check-certificate --content-disposition https://raw.githubusercontent.com/vanderlei-test/dataset/master/reto-4-compu-train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'reto-4-compu-train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acerca del Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las variables son todas numericas. Solo nuestra variable TARGET (Prestamo aprobado o posible incumplimiento financiero) es the tipo float.\n",
    "\n",
    "La función describe() de abajo muestra varias estadisticas del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento y evaluación de un modelo de clasificación binaria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Transformación 1: excluir columnas del conjunto de datos\n",
    "\n",
    "Para la creación de una transformación de datos personalizada en scikit-learn, es necesario crear una clase con los métodos transform y fit. En el método de 'transform', se ejecutará la lógica de nuestra transformación.\n",
    "\n",
    "La siguiente celda muestra el código completo de una transformación DropColumns para eliminar columnas de un pandas DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "# All sklearn Transforms must have the `transform` and `fit` methods\n",
    "class DropColumns(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Primero copiamos el dataframe de datos de entrada 'X'\n",
    "        data = X.copy()\n",
    "        # Devolvemos un nuevo dataframe de datos sin las columnas no deseadas\n",
    "        return data.drop(labels=self.columns, axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación de instancias de una transformación DropColumns\n",
    "rm_columns = DropColumns(\n",
    "    columns=[\"CXC\", \"CXP\"]  # Esta transformación toma como parámetro una lista con los nombres de las columnas no deseadas\n",
    ")\n",
    "\n",
    "print(rm_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver las columnas del conjunto de datos original\n",
    "print(\"Columnas del conjunto de datos original: \\n\")\n",
    "print(df.columns)\n",
    "\n",
    "# Aplicar la transformación ``DropColumns`` al conjunto de datos base\n",
    "rm_columns.fit(X=df)\n",
    "\n",
    "# Reconstruyendo un DataFrame de Pandas con el resultado de la transformación\n",
    "df2 = pd.DataFrame.from_records(\n",
    "    data=rm_columns.transform(\n",
    "        X=df\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Ver las columnas del conjunto de datos transformado\n",
    "print(\"\\n\\nColumnas del conjunto de datos después de la transformación ``DropColumns``: \\n\")\n",
    "print(df2.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformación 2: estandarización de Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definición de features (Variables Independientes)\n",
    "\n",
    "En este * ejemplo * usaremos todas las columnas. (Usted debe decidir cuales variables utilizar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### División en 80% entrenamiento y 20% pruebas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformación 3: tratamiento de datos faltantes\n",
    "\n",
    "Para manejar los datos que faltan en nuestro conjunto de datos, ahora usaremos una transformación lista para usar de la biblioteca scikit-learn, llamada SimpleImputer.\n",
    "\n",
    "Esta transformación permite varias estrategias para el tratamiento de datos faltantes. La documentación oficial se puede encontrar en: https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html\n",
    "\n",
    "En este ejemplo, simplemente haremos cero todos los valores faltante usted puede escoger otra estrategia ;)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un objeto ``SimpleImputer``\n",
    "si = SimpleImputer(\n",
    "    missing_values=np.nan,  # los valores que faltan son del tipo ``np.nan`` (Pandas estándar)\n",
    "    strategy='constant',  # la estrategia elegida es cambiar el valor faltante por una constante\n",
    "    fill_value=0,  # la constante que se usará para completar los valores faltantes es un int64 = 0\n",
    "    verbose=0,\n",
    "    copy=True\n",
    ")\n",
    "\n",
    "print(si)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver los datos faltantes del conjunto de datos antes de la primera transformación (df_data_2)\n",
    "print(\"Valores nulos antes de la transformación SimpleImputer: \\n\\n{}\\n\".format(df2.isnull().sum(axis = 0)))\n",
    "\n",
    "# Aplicamos el SimpleImputer ``si`` al conjunto de datos df_data_2 (resultado de la primera transformación)\n",
    "si.fit(X=df2)\n",
    "\n",
    "# Reconstrucción de un nuevo DataFrame de Pandas con el conjunto imputado (df_data_3)\n",
    "df3 = pd.DataFrame.from_records(\n",
    "    data=si.transform(\n",
    "        X=df2\n",
    "    ),  # el resultado SimpleImputer.transform (<< pandas dataframe >>) es lista lista\n",
    "    columns=df2.columns  # las columnas originales deben conservarse en esta transformación\n",
    ")\n",
    "\n",
    "# Ver los datos faltantes del conjunto de datos después de la segunda transformación (SimpleImputer) (df_data_3)\n",
    "print(\"\\n\\nValores nulos en el conjunto de datos después de la transformación SimpleImputer: \\n\\n{}\\n\".format(df3.isnull().sum(axis = 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definiendo las variables features y target (removed CXC and CXP)\n",
    "\n",
    "features = df3[\n",
    "    [\n",
    "        'EFECTIVO',\n",
    "        'INVENTARIO',\n",
    "        'EQ_OFICINA',\n",
    "        'EQ_TRANSPORTE',\n",
    "        'TERRENOS_Y_CONSTRUCCIONES',\n",
    "        'CONTRIBUCIONES_X_PAGAR',\n",
    "        'ANTICIPOS_CTE',\n",
    "        'CAP_SOCIAL',\n",
    "        'UTILIDADES_ACUMULADAS',\n",
    "        'UTILIDAD_O_PERDIDA',\n",
    "        'TOTAL_VENTAS',\n",
    "        'TOTAL_COMPRAS',\n",
    "        'UTILIDAD_BRUTA',\n",
    "        'TOTAL_GASTOS',\n",
    "    ]\n",
    "]\n",
    "target = df3[\"OBJETIVO\"]  ## No cambie la variable target!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenando un modelo ``DecisionTreeClassifier()``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Método para creacion de modelos basados en arbol de desición\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dtc = DecisionTreeClassifier(max_depth=3)\n",
    "model = dtc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Haciendo una predicción con el set de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = dtc.predict(X_test)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analizar la calidad del modelo a través de la matriz de confusión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "group_names = ['Aprobado `Aceptado`', 'Refused `Sospechoso`', 'Refused `Aceptado`', 'Aprobado `Sospechoso`']\n",
    "group_counts = ['{0:0.0f}'.format(value) for value in cf_matrix.flatten()]\n",
    "group_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten()/np.sum(cf_matrix)]\n",
    "labels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in zip(group_names, group_counts, group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "accuracy  = np.trace(cf_matrix) / float(np.sum(cf_matrix))\n",
    "precision = cf_matrix[1,1] / sum(cf_matrix[:,1])\n",
    "recall    = cf_matrix[1,1] / sum(cf_matrix[1,:])\n",
    "f1_score  = 2*precision*recall / (precision + recall)\n",
    "sns.heatmap(cf_matrix, annot=labels, fmt='')\n",
    "stats_text = \"\\n\\nAccuracy={:0.3f}\\nPrecision={:0.3f}\\nRecall={:0.3f}\\nF1 Score={}\".format(accuracy, precision, recall, f1_score)\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label' + stats_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ¡ATENCIÓN! Su puntuación en este desafío de clasificación se basará en la puntuación F1 del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construcción del Pipeline completo para el encapsulamiento en WML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparando transformaciones personalizadas para cargar en WML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el reto 2 (Tortuga Code), se mostró cómo crear una transformación personalizada, declarando una clase Python con los métodos ``fit`` y ``transform``.\n",
    "\n",
    "    - Código de transformación personalizada DropColumns():\n",
    "    \n",
    "    from sklearn.base import BaseEstimator, TransformerMixin\n",
    "    # All sklearn Transforms must have the `transform` and `fit` methods\n",
    "    class DropColumns(BaseEstimator, TransformerMixin):\n",
    "        def __init__(self, columns):\n",
    "            self.columns = columns\n",
    "        def fit(self, X, y=None):\n",
    "            return self\n",
    "        def transform(self, X):\n",
    "            # Primero copiamos el dataframe de entrada 'X' de entrada\n",
    "            data = X.copy()\n",
    "            # Devolvemos un nuevo marco de datos sin las columnas no deseadas\n",
    "            return data.drop(labels=self.columns, axis='columns')\n",
    "\n",
    "Para integrar estos tipos de transformaciones personalizadas con Pipelines en Watson Machine Learning, primero debe empaquetar su código personalizado como una biblioteca de Python. Esto se puede hacer fácilmente usando la herramienta *setuptools*.\n",
    "\n",
    "En el siguiente repositorio de git: https://github.com/vnderlev/sklearn_transforms tenemos todos los archivos necesarios para crear un paquete de Python, llamado **my_custom_sklearn_transforms**.\n",
    "Este paquete tiene la siguiente estructura de archivos:\n",
    "\n",
    "    /my_custom_sklearn_transforms.egg-info\n",
    "        dependency_links.txt\n",
    "        not-zip-safe\n",
    "        PKG-INFO\n",
    "        SOURCES.txt\n",
    "        top_level.txt\n",
    "    /my_custom_sklearn_transforms\n",
    "        __init__.py\n",
    "        sklearn_transformers.py\n",
    "    PKG-INFO\n",
    "    README.md\n",
    "    setup.cfg\n",
    "    setup.py\n",
    "    \n",
    "El archivo principal, que contendrá el código para nuestras transformaciones personalizadas, es el archivo **/my_custom_sklearn_transforms/sklearn_transformers.py**. Si accedes a él en el repositorio, notarás que contiene exactamente el mismo código declarado en el primer paso (la clase DropColumns).\n",
    "\n",
    "Si has declarado sus propias transformaciones (además de la DropColumn proporcionada), debes agregar todas las clases de esas transformaciones creadas en este mismo archivo. Para hacer esto, debes hacer fork de este repositorio (esto se puede hacer en la propia interfaz web de Github, haciendo clic en el botón como se muestra en la imagen a continuación) y agregue sus clases personalizadas al archivo **sklearn_transformers.py**.\n",
    "\n",
    "![alt text](https://i.imgur.com/2lZ4Ty2.png \"forking-a-repo\")\n",
    "\n",
    "Si solo hizo uso de la transformación proporcionada (DropColumns), puede omitir este paso de fork y continuar usando el paquete base provisto. :)\n",
    "\n",
    "Después de preparar su paquete de Python con sus transformaciones personalizadas, reemplace el enlace del repositorio de git en la celda a continuación y ejecútelo. Si no ha preparado ninguna transformación nueva, ejecute la celda con el enlace del repositorio ya proporcionado.\n",
    "\n",
    "<hr>\n",
    "    \n",
    "**OBSERVACIÓN**\n",
    "\n",
    "Si la ejecución de la celda a continuación devuelve un error de que el repositorio ya existe, ejecute:\n",
    "\n",
    "**!rm -r -f sklearn_transforms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Susbtituya el link de abajo por el link de su repositorio git (se es necesario)\n",
    "!git clone https://github.com/vnderlev/sklearn_transforms.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd sklearn_transforms\n",
    "!ls -ltr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para subir o código no WML, precisamos enviar um arquivo .zip com todo o código fonte, então iremos zipar o diretório clonado em seguida:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!zip -r sklearn_transforms.zip sklearn_transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el archivo zip de nuestro paquete cargado en el Kernel de este notebook, podemos utiliar la herramienta pip para instalarlo conforme a la siguiente celda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sklearn_transforms.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos realizar la importación de nuestro paquiete personalizado en nuestro notabook!\n",
    "\n",
    "Vamos a importan la transformación DropColumns. Si usted posee otras transformaciones personalizadas, ahora es que debe importarlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_custom_sklearn_transforms.sklearn_transformers import DropColumns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construyendo el model Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATENCIÓN, NO CAMBIE LA CELDA DE ABAJO O SU MODELO NO SERA EVALUADO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_input = df[\n",
    "    [\n",
    "        'EFECTIVO',\n",
    "        'CXC',\n",
    "        'INVENTARIO',\n",
    "        'EQ_OFICINA',\n",
    "        'EQ_TRANSPORTE',\n",
    "        'TERRENOS_Y_CONSTRUCCIONES',\n",
    "        'CXP',\n",
    "        'CONTRIBUCIONES_X_PAGAR',\n",
    "        'ANTICIPOS_CTE',\n",
    "        'CAP_SOCIAL',\n",
    "        'UTILIDADES_ACUMULADAS',\n",
    "        'UTILIDAD_O_PERDIDA',\n",
    "        'TOTAL_VENTAS',\n",
    "        'TOTAL_COMPRAS',\n",
    "        'UTILIDAD_BRUTA',\n",
    "        'TOTAL_GASTOS',\n",
    "    ]\n",
    "]\n",
    "\n",
    "pipeline_target = df['OBJETIVO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separación de los datos en un set de entrenamiento y otro de prueba (PARA CREACION DEL PIPELINE)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    pipeline_input,\n",
    "    pipeline_target,\n",
    "    test_size=0.3,\n",
    "    random_state=21233\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Borrando las columnas del dataset original\n",
    "\n",
    "Debe elminiar todas las columnas que no esta usando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creacion de la Transformación Personalizada ``DropColumns``\n",
    "\n",
    "rm_columns = DropColumns(\n",
    "    columns=['CXC', 'CXP']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reemplazando con zeros en lugar de valores nulos\n",
    "\n",
    "Ud puede usar otras estrategias, pero deben ser con Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un objeto ``SimpleImputer``\n",
    "\n",
    "si = SimpleImputer(\n",
    "    missing_values=np.nan,  # los valores que faltan son del tipo ``np.nan`` (Pandas estándar)\n",
    "    strategy='constant',  # la estrategia elegida es cambiar el valor faltante por una constante (Ejemplo)\n",
    "    fill_value=0,  # la constante que se usará para completar los valores faltantes es un int64 = 0\n",
    "    verbose=0,\n",
    "    copy=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estandarizando los valores numericos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-entrenando el modelo para definir el pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la celda de abajo se declara un objeto **Pipeline** de scikit-learn, donde es declarado como parametros *steps*, que es una lista de etapas a ejecutar el pipeline:\n",
    "\n",
    "    'paso_1_remove_cols'     - Transformación personalizada DropColumns\n",
    "    'paso_2_imputer'         - Transformación embebida de scikit-learn para remplazar los valores faltantes\n",
    "    'paso_3_standard_scaler'          - Transformación embebida de scikit-learn para escalar las variables numéricas\n",
    "    'su_modelo'              - Un árbol de desición simple\n",
    "    \n",
    "Note que pasamos como pasos las transformaciones instanciadas anteriormente, con nombres `rm_columns` y `si`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación de nuestro para almacenamiento en Watson Machine Learning:\n",
    "my_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        ('paso_1_remove_cols', rm_columns),\n",
    "        ('paso_2_imputer', si),\n",
    "        ('paso_3_standard_scaler', sc),\n",
    "        ('su_modelo', DecisionTreeClassifier(max_depth=3)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En seguida ejecutaremos el método `fit()` del Pipeline, realizando el pré-procesamiento y el entrenamiento del modelo de una sola vez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializando el Pipeline (pre-procesamiento y entrenamiento del modelo)\n",
    "my_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Haciendo una predicción con el set de prueba\n",
    "\n",
    "y_pred = my_pipeline.predict(X_test)\n",
    "cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "group_names = ['Aprobado `Aceptado`', 'Refused `Sospechoso`', 'Refused `Aceptado`', 'Aprobado `Sospechoso`']\n",
    "group_counts = ['{0:0.0f}'.format(value) for value in cf_matrix.flatten()]\n",
    "group_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten()/np.sum(cf_matrix)]\n",
    "labels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in zip(group_names, group_counts, group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "accuracy  = np.trace(cf_matrix) / float(np.sum(cf_matrix))\n",
    "precision = cf_matrix[1,1] / sum(cf_matrix[:,1])\n",
    "recall    = cf_matrix[1,1] / sum(cf_matrix[1,:])\n",
    "f1_score  = 2*precision*recall / (precision + recall)\n",
    "sns.heatmap(cf_matrix, annot=labels, fmt='')\n",
    "stats_text = \"\\n\\nAccuracy={:0.3f}\\nPrecision={:0.3f}\\nRecall={:0.3f}\\nF1 Score={:0.3f}\".format(accuracy, precision, recall, f1_score)\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label' + stats_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precisión simple\n",
    "my_pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que tenemos un Pipeline completo, con etapas de pre-procesamiento configuradas y tambien un modelo por Árbol de Desición entrenado, podemos realizar la integración con Watson Machine Learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encapsulando un Pipeline personalizado de Watson Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estableciendo conexión entre el cliente Python de WML y su instancia del servicio en la nube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biblioteca Python con implementación de un cliente HTTP para la API de WML\n",
    "from watson_machine_learning_client import WatsonMachineLearningAPIClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las próximas celdas realizaran el despliegue del pipeline declarada en este notebook en WML. Solo prosiga si usted ya está satisfecho con su modelo y cree que ya es hora de hacer el despliegue de su solución.\n",
    "\n",
    "Copie las credenciales de su instancia de Watson Machine Learning en la variable de la celda de abajo.\n",
    "\n",
    "Es importante que la variable que contenga los valores de la credencial se llame ``wml_credentials`` para que las proximas celdas de este notebook se ejecuten corretamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wml_credentials = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciando un objeto cliente de Watson Machine Learning a partir de las credenciales\n",
    "\n",
    "clientWML = WatsonMachineLearningAPIClient(wml_credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrayendo los detalles de su de Watson Machine Learning\n",
    "\n",
    "instance_details = clientWML.service_instance.get_details()\n",
    "print(json.dumps(instance_details, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¡¡ATENCIÓN!!**\n",
    "\n",
    "¡Este atento de los limites de consumo de su instancia de Watson Machine Learning!\n",
    "\n",
    "En caso de que acabe la capa gratuita, no sera posible evualuar su modelo (Pues es necesario para la realización de algunas llamadasal API con sus predicciones!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Listando todos los artefatos almacenados en su WML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para listar todos los artefatos almacenados en su Watson Machine Learning, usted puede usar la seguinte función:\n",
    "\n",
    "    clientWML.repository.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listando todos los artefatos actualmente almacenados en su instancia de WML\n",
    "\n",
    "clientWML.repository.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ATENCIÓN!, SI UD CORRE LAS CELDAS DE ABAJO TODOS LOS DESPLIEGUES ANTERIORS SERAN BORRADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Celda para borrar los Deployments:\n",
    "\n",
    "for uid in clientWML.deployments.get_uids():\n",
    "    if clientWML.deployments.get_details(uid)['entity']['name'] == 'deployment_meta_4' : \n",
    "        print('Deleted ' + clientWML.deployments.get_details(uid)['entity']['name'] )\n",
    "        clientWML.deployments.delete(uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Celda para borrar todos los recursos:\n",
    "\n",
    "d = clientWML.repository.get_details()\n",
    "for k in d:\n",
    "    for res in d[k][\"resources\"]:\n",
    "        if res['entity']['name'] in ['package_meta_4', 'runtime_meta_4', 'pipeline_meta_4']:\n",
    "            clientWML.repository.delete(res[\"metadata\"][\"guid\"])\n",
    "            print('Deleted ' + res['entity']['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listando todos los artefatos actualmente almacenados en su instancia de WML\n",
    "\n",
    "clientWML.repository.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creando una nueva definición de paquete Python personalizado en WML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El primer paso para realizar su deploy y almacenar el código de las transformaciones personalizadas creadas por usted.\n",
    "\n",
    "Para esta etapa solo necesitamos el archivo .zip del paquete creado por usted (Que ya tenemos cargados en el Kernel!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de metadatos del paquete con las Transformaciones personalizadas\n",
    "pkg_meta = {\n",
    "    clientWML.runtimes.LibraryMetaNames.NAME: \"package_meta_4\",\n",
    "    clientWML.runtimes.LibraryMetaNames.DESCRIPTION: \"A custom sklearn transform package\",\n",
    "    clientWML.runtimes.LibraryMetaNames.FILEPATH: \"sklearn_transforms.zip\",  # Note que estamos utilizando o .zip criado anteriormente!\n",
    "    clientWML.runtimes.LibraryMetaNames.VERSION: \"1.0\",\n",
    "    clientWML.runtimes.LibraryMetaNames.PLATFORM: { \"name\": \"python\", \"versions\": [\"3.6\"] }\n",
    "}\n",
    "custom_package_details = clientWML.runtimes.store_library( pkg_meta )\n",
    "custom_package_uid = clientWML.runtimes.get_library_uid( custom_package_details )\n",
    "\n",
    "print(\"\\n Lista de artefactos de runtime almacenados en WML:\")\n",
    "clientWML.repository.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creando una nueva definición de runtime Python personalizado en WML\n",
    "\n",
    "El segundo paso es almacenar una definición de runtime Python para utilizar en nuestra biblioteca personalizada.\n",
    "\n",
    "Esto puede hacerse de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_meta = {\n",
    "    clientWML.runtimes.ConfigurationMetaNames.NAME: \"runtime_meta_4\",\n",
    "    clientWML.runtimes.ConfigurationMetaNames.DESCRIPTION: \"A Python runtime with custom sklearn Transforms\",\n",
    "    clientWML.runtimes.ConfigurationMetaNames.PLATFORM: {\n",
    "        \"name\": \"python\",\n",
    "        \"version\": \"3.6\"\n",
    "    },\n",
    "    clientWML.runtimes.ConfigurationMetaNames.LIBRARIES_UIDS: [ custom_package_uid ]\n",
    "}\n",
    "runtime_details = clientWML.runtimes.store( runtime_meta )\n",
    "custom_runtime_uid = clientWML.runtimes.get_uid( runtime_details )\n",
    "\n",
    "print(\"\\n Detalles del runtime almacenados:\")\n",
    "print(json.dumps(runtime_details, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listando todos los runtimes almacenados en su WML:\n",
    "clientWML.runtimes.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creando una nueva definición de Pipeline personalizado en WML\n",
    "\n",
    "Finalmente creando una definición (metadatos) para que nuestro Pipeline sea hospedada en WML.\n",
    "\n",
    "Definimos como parametros el nombre para el artefacto y el ID de runtime creado anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_meta = {\n",
    "    clientWML.repository.ModelMetaNames.NAME: 'pipeline_meta_4',\n",
    "    clientWML.repository.ModelMetaNames.DESCRIPTION: \"my pipeline for submission\",\n",
    "    clientWML.repository.ModelMetaNames.RUNTIME_UID: custom_runtime_uid\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En seguida llamamos el método para almacenar una nueva definición:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para almacenar una definición de Pipeline en WML\n",
    "stored_model_details = clientWML.repository.store_model(\n",
    "    model=my_pipeline,  # `my_pipeline` es la variable creada anteriormente que contiene nuestro Pipeline ya entrenado :)\n",
    "    meta_props=model_meta,  # Metadatos definidos en la celda anterior\n",
    "    training_data=None  # No altere este parametro\n",
    ")\n",
    "\n",
    "print(\"\\n Lista de artefatos almacenados en WML:\")\n",
    "clientWML.repository.list()\n",
    "\n",
    "# Datalles del modelo hospedado en Watson Machine Learning\n",
    "print(\"\\n Metadatos del modelo almacenado:\")\n",
    "print(json.dumps(stored_model_details, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Realizando un deployment de su modelo para consumo inmediato por otras aplicaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El deployment del modelo es finalmente realizado por medio del método ``deployments.create()``\n",
    "\n",
    "model_deployment_details = clientWML.deployments.create(\n",
    "    artifact_uid=stored_model_details[\"metadata\"][\"guid\"],  # No altere este parametro\n",
    "    name=\"deployment_meta_4\",\n",
    "    description=\"Desafio 4 MBTC\",\n",
    "    asynchronous=False,  # No altere este parametro\n",
    "    deployment_type='online',  # No altere este parametro\n",
    "    deployment_format='Core ML',  # No altere este parametro\n",
    "    meta_props=model_meta  # No altere este parametro\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probando el modelo hospedado en Watson Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recuperando la URL endpoint dl modelo hospedado en la celda anterior\n",
    "\n",
    "model_endpoint_url = clientWML.deployments.get_scoring_url(model_deployment_details)\n",
    "print(\"La URL de llamada de su API es: {}\".format(model_endpoint_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ATENCIÓN: UD necesitará de la URL de arriba para entregar su modelo :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detalles del deployment realizado\n",
    "\n",
    "deployment_details = clientWML.deployments.get_details(\n",
    "    deployment_uid=model_deployment_details[\"metadata\"][\"guid\"]  # Este es el ID de su deployment!\n",
    ")\n",
    "\n",
    "print(\"Metadatos del deployment realizado: \\n\")\n",
    "print(json.dumps(deployment_details, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realizando una llamada de API para su modelo almacenado en WML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scoring_payload = {\n",
    "    'fields': [\n",
    "        'EFECTIVO',\n",
    "        'CXC',\n",
    "        'INVENTARIO',\n",
    "        'EQ_OFICINA',\n",
    "        'EQ_TRANSPORTE',\n",
    "        'TERRENOS_Y_CONSTRUCCIONES',\n",
    "        'CXP',\n",
    "        'CONTRIBUCIONES_X_PAGAR',\n",
    "        'ANTICIPOS_CTE',\n",
    "        'CAP_SOCIAL',\n",
    "        'UTILIDADES_ACUMULADAS',\n",
    "        'UTILIDAD_O_PERDIDA',\n",
    "        'TOTAL_VENTAS',\n",
    "        'TOTAL_COMPRAS',\n",
    "        'UTILIDAD_BRUTA',\n",
    "        'TOTAL_GASTOS',\n",
    "    ],\n",
    "    'values': [\n",
    "        [\n",
    "            968866.8993,\n",
    "            102102.000,\n",
    "            8539205.63,\n",
    "            3898282.548,\n",
    "            416873.3265,\n",
    "            1420050.089,\n",
    "            -629717.8548,\n",
    "            14613560.64,\n",
    "            7620711.462,\n",
    "            116647.7396,\n",
    "            1798064.624,\n",
    "            9535423.826,\n",
    "            3657339.603,\n",
    "            770284.5004,\n",
    "            -102101.201,\n",
    "            -711032.0155\n",
    "        ]\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"\\n Payload de datos a ser clasificado:\")\n",
    "print(json.dumps(scoring_payload, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = clientWML.deployments.score(\n",
    "    model_endpoint_url,\n",
    "    scoring_payload\n",
    ")\n",
    "\n",
    "print(\"\\n Resultados:\")\n",
    "print(json.dumps(result, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## ¡Felicitaciones! \n",
    "\n",
    "Si todo fue ejecutado sin errores, ¡usted ya tiene un predictor basado en clasificacíon binaria encapsulado como una API REST!\n",
    "\n",
    "Para enviar su solución, accede a la página:\n",
    "\n",
    "# https://compusoluciones.maratona.dev\n",
    "\n",
    "Usted necesitará del endpoint url de su modelo y las credenciales de WML :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
