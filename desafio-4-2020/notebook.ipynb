{"cells": [{"metadata": {"collapsed": true}, "cell_type": "markdown", "source": "# MARATONA BEHIND THE CODE 2020\n\n## DESAFIO 4 - CompuSoluciones"}, {"metadata": {}, "cell_type": "markdown", "source": "### Introducci\u00f3n"}, {"metadata": {}, "cell_type": "markdown", "source": "En este desaf\u00edo, CompuSoluciones usar\u00e1 herramientas de IBM como Watson Studio (o Cloud Pack for Data) y Watson Machine Learning para construir un modelo de Machine Learning natural capaz de predecir la probabilidad de cumplimiento de pago.\n\nLa idea esencial del Desaf\u00edo 4 es crear un modelo basado en machine learning capaz de identificar el comportamiento financiero del asociado de negocio, permitiendo una probabilidad de cumplimiento o incumplimiento del cr\u00e9dito."}, {"metadata": {}, "cell_type": "markdown", "source": "<hr>"}, {"metadata": {}, "cell_type": "markdown", "source": "### Instalaci\u00f3n de Librerias"}, {"metadata": {}, "cell_type": "code", "source": "!pip install scikit-learn --upgrade", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Si desea utilizar la biblioteca ** xgboost **, instale la versi\u00f3n 0.71.\n#!pip install xgboost==0.71 --upgrade", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "import json\nimport requests\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nfrom datetime import datetime\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import KFold\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nimport matplotlib.pyplot as plt\nimport seaborn as sns", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Descargando el dataset csv desde Github"}, {"metadata": {}, "cell_type": "code", "source": "!wget --no-check-certificate --content-disposition https://raw.githubusercontent.com/vanderlei-test/dataset/master/reto-4-compu-train.csv", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "df = pd.read_csv(r'reto-4-compu-train.csv')\ndf.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Acerca del Dataset"}, {"metadata": {"scrolled": false}, "cell_type": "code", "source": "df.info()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Las variables son todas numericas. Solo nuestra variable TARGET (Prestamo aprobado o posible incumplimiento financiero) es the tipo float.\n\nLa funci\u00f3n describe() de abajo muestra varias estadisticas del dataset."}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "df.describe()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<hr>"}, {"metadata": {}, "cell_type": "markdown", "source": "### Entrenamiento y evaluaci\u00f3n de un modelo de clasificaci\u00f3n binaria"}, {"metadata": {}, "cell_type": "markdown", "source": "\n#### Transformaci\u00f3n 1: excluir columnas del conjunto de datos\n\nPara la creaci\u00f3n de una transformaci\u00f3n de datos personalizada en scikit-learn, es necesario crear una clase con los m\u00e9todos transform y fit. En el m\u00e9todo de 'transform', se ejecutar\u00e1 la l\u00f3gica de nuestra transformaci\u00f3n.\n\nLa siguiente celda muestra el c\u00f3digo completo de una transformaci\u00f3n DropColumns para eliminar columnas de un pandas DataFrame.\n"}, {"metadata": {}, "cell_type": "code", "source": "from sklearn.base import BaseEstimator, TransformerMixin\n\n\n# All sklearn Transforms must have the `transform` and `fit` methods\nclass DropColumns(BaseEstimator, TransformerMixin):\n    def __init__(self, columns):\n        self.columns = columns\n\n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        # Primero copiamos el dataframe de datos de entrada 'X'\n        data = X.copy()\n        # Devolvemos un nuevo dataframe de datos sin las columnas no deseadas\n        return data.drop(labels=self.columns, axis='columns')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Creaci\u00f3n de instancias de una transformaci\u00f3n DropColumns\nrm_columns = DropColumns(\n    columns=[\"CXC\", \"CXP\"]  # Esta transformaci\u00f3n toma como par\u00e1metro una lista con los nombres de las columnas no deseadas\n)\n\nprint(rm_columns)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Ver las columnas del conjunto de datos original\nprint(\"Columnas del conjunto de datos original: \\n\")\nprint(df.columns)\n\n# Aplicar la transformaci\u00f3n ``DropColumns`` al conjunto de datos base\nrm_columns.fit(X=df)\n\n# Reconstruyendo un DataFrame de Pandas con el resultado de la transformaci\u00f3n\ndf2 = pd.DataFrame.from_records(\n    data=rm_columns.transform(\n        X=df\n    ),\n)\n\n# Ver las columnas del conjunto de datos transformado\nprint(\"\\n\\nColumnas del conjunto de datos despu\u00e9s de la transformaci\u00f3n ``DropColumns``: \\n\")\nprint(df2.columns)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "#### Transformaci\u00f3n 3: tratamiento de datos faltantes\n\nPara manejar los datos que faltan en nuestro conjunto de datos, ahora usaremos una transformaci\u00f3n lista para usar de la biblioteca scikit-learn, llamada SimpleImputer.\n\nEsta transformaci\u00f3n permite varias estrategias para el tratamiento de datos faltantes. La documentaci\u00f3n oficial se puede encontrar en: https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html\n\nEn este ejemplo, simplemente haremos cero todos los valores faltante usted puede escoger otra estrategia ;)."}, {"metadata": {}, "cell_type": "code", "source": "# Crear un objeto ``SimpleImputer``\nsi = SimpleImputer(\n    missing_values=np.nan,  # los valores que faltan son del tipo ``np.nan`` (Pandas est\u00e1ndar)\n    strategy='constant',  # la estrategia elegida es cambiar el valor faltante por una constante\n    fill_value=0,  # la constante que se usar\u00e1 para completar los valores faltantes es un int64 = 0\n    verbose=0,\n    copy=True\n)\n\nprint(si)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Ver los datos faltantes del conjunto de datos antes de la primera transformaci\u00f3n (df_data_2)\nprint(\"Valores nulos antes de la transformaci\u00f3n SimpleImputer: \\n\\n{}\\n\".format(df2.isnull().sum(axis = 0)))\n\n# Aplicamos el SimpleImputer ``si`` al conjunto de datos df_data_2 (resultado de la primera transformaci\u00f3n)\nsi.fit(X=df2)\n\n# Reconstrucci\u00f3n de un nuevo DataFrame de Pandas con el conjunto imputado (df_data_3)\ndf3 = pd.DataFrame.from_records(\n    data=si.transform(\n        X=df2\n    ),  # el resultado SimpleImputer.transform (<< pandas dataframe >>) es lista lista\n    columns=df2.columns  # las columnas originales deben conservarse en esta transformaci\u00f3n\n)\n\n# Ver los datos faltantes del conjunto de datos despu\u00e9s de la segunda transformaci\u00f3n (SimpleImputer) (df_data_3)\nprint(\"\\n\\nValores nulos en el conjunto de datos despu\u00e9s de la transformaci\u00f3n SimpleImputer: \\n\\n{}\\n\".format(df3.isnull().sum(axis = 0)))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "#### Definici\u00f3n de features (Variables Independientes)\n\nEn este * ejemplo * usaremos todas las columnas. (Usted debe decidir cuales variables utilizar)"}, {"metadata": {}, "cell_type": "code", "source": "# Definiendo las variables features y target (removed CXC and CXP)\n\nfeatures = df3[\n    [\n        'EFECTIVO',\n        'INVENTARIO',\n        'EQ_OFICINA',\n        'EQ_TRANSPORTE',\n        'TERRENOS_Y_CONSTRUCCIONES',\n        'CONTRIBUCIONES_X_PAGAR',\n        'ANTICIPOS_CTE',\n        'CAP_SOCIAL',\n        'UTILIDADES_ACUMULADAS',\n        'UTILIDAD_O_PERDIDA',\n        'TOTAL_VENTAS',\n        'TOTAL_COMPRAS',\n        'UTILIDAD_BRUTA',\n        'TOTAL_GASTOS',\n    ]\n]\ntarget = df3[\"OBJETIVO\"]  ## No cambie la variable target!", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "#### Divisi\u00f3n en 80% entrenamiento y 20% pruebas"}, {"metadata": {}, "cell_type": "code", "source": "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=None)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "#### Entrenando un modelo ``DecisionTreeClassifier()``"}, {"metadata": {}, "cell_type": "code", "source": "# M\u00e9todo para creacion de modelos basados en arbol de desici\u00f3n\nfrom sklearn.tree import DecisionTreeClassifier\n\ndtc_model = DecisionTreeClassifier(max_depth=3)\nmodel = dtc_model.fit(X_train, y_train)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "#### Haciendo una predicci\u00f3n con el set de prueba"}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "y_pred = dtc_model.predict(X_test)\nprint(y_pred)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "#### Analizar la calidad del modelo a trav\u00e9s de la matriz de confusi\u00f3n"}, {"metadata": {}, "cell_type": "code", "source": "\nfrom sklearn.metrics import confusion_matrix\n\ncf_matrix = confusion_matrix(y_test, y_pred)\ngroup_names = ['Aprobado `Aceptado`', 'Refused `Sospechoso`', 'Refused `Aceptado`', 'Aprobado `Sospechoso`']\ngroup_counts = ['{0:0.0f}'.format(value) for value in cf_matrix.flatten()]\ngroup_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten()/np.sum(cf_matrix)]\nlabels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in zip(group_names, group_counts, group_percentages)]\nlabels = np.asarray(labels).reshape(2,2)\naccuracy  = np.trace(cf_matrix) / float(np.sum(cf_matrix))\nprecision = cf_matrix[1,1] / sum(cf_matrix[:,1])\nrecall    = cf_matrix[1,1] / sum(cf_matrix[1,:])\nf1_score  = 2*precision*recall / (precision + recall)\nsns.heatmap(cf_matrix, annot=labels, fmt='')\nstats_text = \"\\n\\nAccuracy={:0.3f}\\nPrecision={:0.3f}\\nRecall={:0.3f}\\nF1 Score={}\".format(accuracy, precision, recall, f1_score)\nplt.ylabel('True label')\nplt.xlabel('Predicted label' + stats_text)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "##### \u00a1ATENCI\u00d3N! Su puntuaci\u00f3n en este desaf\u00edo de clasificaci\u00f3n se basar\u00e1 en la puntuaci\u00f3n F1 del modelo al predecir el dataset de evaluaci\u00f3n."}, {"metadata": {}, "cell_type": "markdown", "source": "<hr>"}, {"metadata": {}, "cell_type": "markdown", "source": "## Puntuaci\u00f3n de los datos necesarios para entregar la soluci\u00f3n\n\nComo entrega de su soluci\u00f3n, esperamos que los resultados se clasifiquen en el siguiente conjunto de datos llamado \"to_be_scored_compusoluciones.csv\":\n\n### Descarga la \"hoja de evaluaci\u00f3n\""}, {"metadata": {}, "cell_type": "code", "source": "!wget --no-check-certificate --content-disposition https://raw.githubusercontent.com/vanderlei-test/dataset/master/to_be_scored_compusoluciones.csv\ndf_to_be_scored = pd.read_csv(r'to_be_scored_compusoluciones.csv')\ndf_to_be_scored.tail()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# \u00a1Atenci\u00f3n!\n\nEl Dataframe ``to_be_scored_compusoluciones`` es su \"hoja de evaluaci\u00f3n\". Tenga en cuenta que a la columna \"OBJETIVO\" le faltan datos en este ejemplo, que luego no se pueden usar para entrenar modelos de aprendizaje supervisado."}, {"metadata": {}, "cell_type": "code", "source": "df_to_be_scored.info()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<hr>\n\n# \u00a1Atenci\u00f3n!\n\n# Para aplicar su modelo y clasificar la hoja de evaluaci\u00f3n, primero debe aplicar las mismas transformaciones de columnas que aplic\u00f3 al conjunto de datos de entrenamiento.\n\n# No elimine ni agregue l\u00edneas a la hoja de respuestas.\n\n# No cambie el orden de las l\u00edneas en la hoja de respuestas.\n\n# Al final, se deben clasificar las 600 entradas, con los valores calculados en la columna \"target\"\n\n<hr>"}, {"metadata": {}, "cell_type": "markdown", "source": "En la celda de abajo, repetimos r\u00e1pidamente los mismos pasos de preprocesamiento usados \u200b\u200ben el ejemplo dado con el \u00e1rbol de decisiones"}, {"metadata": {}, "cell_type": "code", "source": "# Aplicar la transformaci\u00f3n ``DropColumns`` al conjunto de datos base\nrm_columns.fit(X=df_to_be_scored)\n\n# Reconstruyendo un DataFrame de Pandas con el resultado de la transformaci\u00f3n\ndf_to_be_scored_2 = pd.DataFrame.from_records(\n    data=rm_columns.transform(\n        X=df_to_be_scored\n    ),\n)\n\n# Aplicamos el SimpleImputer ``si`` al conjunto de datos df_data_2 (resultado de la primera transformaci\u00f3n)\nsi.fit(X=df_to_be_scored_2)\n\n# Reconstrucci\u00f3n de un nuevo DataFrame de Pandas con el conjunto imputado (df_data_3)\ndf_to_be_scored_3 = pd.DataFrame.from_records(\n    data=si.transform(\n        X=df_to_be_scored_2\n    ),  # el resultado SimpleImputer.transform (<< pandas dataframe >>) es lista lista\n    columns=df_to_be_scored_2.columns  # las columnas originales deben conservarse en esta transformaci\u00f3n\n)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Realizaci\u00f3n de una prueba a ciegas en el modelo creado\ny_pred = dtc_model.predict(df_to_be_scored_3)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Agregando las respuestas en la columna \"target\"\ndf_to_be_scored_3['target'] = y_pred\ndf_to_be_scored_3.tail()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Guardar la hoja de respuestas como un archivo .csv para enviar\n**SI YA TIENE UN DATA ASSET CON EL NOMBRE `results.csv` EN ESTE PROYECTO O EN OTRO PROYECTO DE WATSON STUDIO DEBE BORRAR EL ARCHIVO ANTES DE CORRER LA SIGUIENTE CELDA O TENDRA EL ERROR: _RuntimeError: File 'results.csv' already exists in storage._**"}, {"metadata": {}, "cell_type": "code", "source": "project.save_data(file_name=\"results.csv\", data=df_to_be_scored_3.to_csv(index=False))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# Atenci\u00f3n\n\n# La ejecuci\u00f3n de la celda anterior crear\u00e1 un nuevo \"data asset\" en su proyecto de Watson Studio. Deber\u00e1 descargar este archivo junto con este cuaderno y crear un archivo zip con **results.csv** y **notebook.ipynb** para enviarlo. (los archivos deben tener este nombre)"}, {"metadata": {}, "cell_type": "markdown", "source": "<hr>\n\n## \u00a1Felicitaciones! \n\nSi ya est\u00e1 satisfecho con su soluci\u00f3n, vaya a la p\u00e1gina siguiente y env\u00ede los archivos necesarios para su env\u00edo.\n\n# https://compusoluciones.maratona.dev"}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.6", "language": "python"}, "language_info": {"name": "python", "version": "3.6.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}