{"cells": [{"metadata": {"collapsed": true}, "cell_type": "markdown", "source": "# MARATONA BEHIND THE CODE 2020\n\n## DESAFIO 6 - ANAHUAC"}, {"metadata": {}, "cell_type": "markdown", "source": "### Introducci\u00f3n"}, {"metadata": {}, "cell_type": "markdown", "source": "En este desafio, usted usar\u00e1 herramientas de IBM como Watson Studio (o Cloud Pak for Data) para construir un modelo baseado en Machine Learning capaz de preveer si un estudante ir\u00e1 continuar o abandonar\u00e1 su curso."}, {"metadata": {}, "cell_type": "markdown", "source": "<hr>"}, {"metadata": {}, "cell_type": "markdown", "source": "## Installing Libs"}, {"metadata": {}, "cell_type": "code", "source": "!pip install scikit-learn --upgrade", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "!pip install xgboost --upgrade", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<hr>"}, {"metadata": {}, "cell_type": "markdown", "source": "## Loading the .csv dataset from GitHub"}, {"metadata": {}, "cell_type": "code", "source": "import pandas as pd", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "!wget --no-check-certificate --content-disposition https://raw.githubusercontent.com/vanderlei-test/dataset2/master/datasets/ForTraining.csv\ndf_base_for_training = pd.read_csv(r'ForTraining.csv')\ndf_base_for_training.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Descripci\u00f3n: La primera tabla mostrada arriba tiene 4 columnas, 3 son features and el target: `Graduado` that has a binary values={Si, No}.\n\nUsted puede, y debe, usar mas data que esta disponible para construir su modelo. Los siguientes archivos .csv presentados:"}, {"metadata": {}, "cell_type": "code", "source": "!wget --no-check-certificate --content-disposition https://raw.githubusercontent.com/vanderlei-test/dataset2/master/datasets/OrdenMaterias.csv\ndf_orden_materias = pd.read_csv(r'OrdenMaterias.csv')\ndf_orden_materias.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "!wget --no-check-certificate --content-disposition https://raw.githubusercontent.com/vanderlei-test/dataset2/master/datasets/TablaConexiones.csv\ndf_tabla_conexiones = pd.read_csv(r'TablaConexiones.csv')\ndf_tabla_conexiones.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "!wget --no-check-certificate --content-disposition https://raw.githubusercontent.com/vanderlei-test/dataset2/master/datasets/TablaTareas.csv\ndf_tabla_tareas = pd.read_csv(r'TablaTareas.csv')\ndf_tabla_tareas.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Overview del Dataset:\n\n    Disponibles para el participante, ecisten 4 tables cargas en DataFrames anteriormente:\n    \n    **df_base_for_training**\n        - ``studentId``\n        ``reducido``\n        ``ciclo``\n        ``Graduado`` --> \u00a1LA VARIABLE OBJETIVO PARA CLASIFICACI\u00d3N BINARIA!\n        \n    **df_orden_materias**\n        ``reducido``\n        ``2017 - 03``\n        ``2017 - 04``\n        ``2017 - 05``\n        ``2017 - 06``\n        ``2017 - 07``\n        ``2017 - 08``\n        ``2018 - 01``\n        ``2018 - 02``\n        ``2018 - 03``\n        ``2018 - 04``\n        ``2018 - 05``\n        ``2018 - 06``\n        ``2018 - 07``\n        ``2018 - 08``\n        ``2019 - 01``\n        ``2019 - 02``\n        ``2019 - 03``\n        ``2019 - 04``\n        ``2019 - 05``\n        ``2019 - 06``\n        ``2019 - 07``\n        ``2019 - 08``\n        ``2020 - 01``\n        ``2020 - 02``\n        ``2020 - 03``\n        ``2020 - 04``\n        ``2020 - 05``\n        ``2020 - 06``\n        \n    **df_tabla_conexiones**\n        - ``studentId``\n        ``ciclo``\n        ``Dias_Conectado``\n        ``Minutos_Promedio``\n        ``Minutos_Total``\n        \n    **df_tabla_tareas**\n        - ``studentId``\n        ``ciclo``\n        ``Calificacion_Promedio``\n        ``Tareas_Puntuales``\n        ``Tareas_No_Entregadas``\n        ``Tareas_Retrasadas``\n        ``Total_Tareas``\n        \nObserve que la variable ``studentId`` aparece en varias tablas.\n\nUsted puede combinar/merge estos datasets como usted desee."}, {"metadata": {}, "cell_type": "code", "source": "print(\"Columnas en *df_base_for_training*:\")\nprint(df_base_for_training.columns)\n\nprint(\"\\Columnas en *df_orden_materias*:\")\nprint(df_orden_materias.columns)\n\nprint(\"\\Columnas en *df_tabla_conexiones*:\")\nprint(df_tabla_conexiones.columns)\n\nprint(\"\\Columnas en *df_tabla_tareas*:\")\nprint(df_tabla_tareas.columns)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "#### \u00a1ATENCI\u00d3N! La columna **target** es  ``Graduado``, presente en el DataFrame \"df_base_for_training\"."}, {"metadata": {}, "cell_type": "markdown", "source": "<hr>"}, {"metadata": {}, "cell_type": "markdown", "source": "## Uniendo DataFrames en Pandas\n\nDocumentaci\u00f3n oficial para Pandas 1.1.0: https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html"}, {"metadata": {}, "cell_type": "markdown", "source": "Como un **ejemplo** de como usar Pandas, camos a unir/merge la informaci\u00f3n de las tablas \"df_base_for_training\" y \"df_tabla_tareas\" a traves de la llave ``studentId``.\n\nUsted puee editar el dataframes manualmente si lo prefiere, usando Microsoft Excel u otros lenguajes. Recuerde insertar la data procesada en IBM Cloud Pak for Data para que pueda entrenar su modelo."}, {"metadata": {}, "cell_type": "code", "source": "df_base_for_training.tail()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "df_tabla_tareas.tail()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# El resultado de esta celda sera la union de los dos anteriores dataframes\n# usando la columna ``studentId`` como llave.\n\ndf = pd.merge(\n    df_base_for_training, df_tabla_tareas, how='inner',\n    on=None, left_on=['studentId', 'ciclo'], right_on=['studentId', 'ciclo'],\n    left_index=False, right_index=False, sort=True,\n    suffixes=('_x', '_y'), copy=True, indicator=False,\n    validate=None\n)\ndf.tail()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Informaci\u00f3n acerca de las columnas del dataset unido\ndf.info()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "De la informaci\u00f3n de arriba ud puede observar que hay valores Null/NaN en algunas de las columnas.\n\nPara que nuestro modelo quede bien entrenado necesitamos procesar estos valores nulos de una forma adecuada.\n\nUsted escogera la mejor estrategia como parte del desaf\u00edo, pero en la siguiente celda encuentra un **ejemplo** the como puede hacer este procesamiento usanto la libreria *scikit-learn*.\n\n<hr>"}, {"metadata": {}, "cell_type": "markdown", "source": "## Pre-procesando el dataset antes de entrenar"}, {"metadata": {}, "cell_type": "markdown", "source": "### Borrando finlas con valores NaN"}, {"metadata": {}, "cell_type": "markdown", "source": "Usando el metodo Pandas DataFrame.dropna() usted puede remover todas las filas que estan indefinidas para la columna ``Graduado``.\n\nDocs: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html"}, {"metadata": {}, "cell_type": "code", "source": "# Visualizando los datos faltantes del dataset antes de la primera transformaci\u00f3n (df_data_2)\nprint(\"Valores nulos antes de la transformaci\u00f3n DropNA: \\n\\n{}\\n\".format(df.isnull().sum(axis = 0)))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Aplicando la funci\u00f3n para borrar todas las filas con valor NaN en la columna ``Graduado``:\ndf2 = df.dropna(axis='index', how='any', subset=['Graduado'])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Visualizando los datos faltantes del dataset antes de la primera transformaci\u00f3n (SimpleImputer) (df_data_3)\nprint(\"Valores nulos antes de la transformaci\u00f3n SimpleImputer: \\n\\n{}\\n\".format(df2.isnull().sum(axis = 0)))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Procesando valores NaN con SimpleImputer de sklearn\n\nPara los otros valores NaN vamos a usar como **ejemplo** la sustituci\u00f3n por la constante 0. \n\nUsted puede escoger cualquier estrategia que crea que es la mejor para esto :)\n\nDocs: https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html?highlight=simpleimputer#sklearn.impute.SimpleImputer"}, {"metadata": {}, "cell_type": "code", "source": "from sklearn.impute import SimpleImputer\nimport numpy as np\n\n\n# Creando un objeto ``SimpleImputer``\nimpute_zeros = SimpleImputer(\n    missing_values=np.nan,  # Los valores faltantes son de tipo ``np.nan`` (estandar Pandas)\n    strategy='constant',  # La estrategia escogida es reemplazar por una constante\n    fill_value=0,  # La constante que ser\u00e1 usada para reemmplazar los valores faltantes es un int64=0.\n    verbose=0,\n    copy=True\n)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Visualizando los datos faltantes del dataset antes de la segunda transformaci\u00f3n (df_data_2)\nprint(\"Valores nulos antes de transformaci\u00f3n SimpleImputer: \\n\\n{}\\n\".format(df2.isnull().sum(axis = 0)))\n\n# Aplicar la transformaci\u00f3n ``SimpleImputer`` en el conjunto de datos base\nimpute_zeros.fit(X=df)\n\n# Reconstrucci\u00f3n del nuevo DataFrame Pandas (df_data_3)\ndf = pd.DataFrame.from_records(\n    data=impute_zeros.transform(\n        X=df\n    ),  # El resultado SimpleImputer.transform(<<pandas dataframe>>) es una lista de listas\n    columns=df.columns  # Las columnas originals deben ser conservadas en esta transformaci\u00f3n\n)\n\n# Visualizndo los datos faltantes del dataset \nprint(\"Valores nulos del dataset despues de la transformaci\u00f3n SimpleImputer: \\n\\n{}\\n\".format(df.isnull().sum(axis = 0)))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Eliminando columnas no desadas\n\nVamos a **demostrar** acontinuaci\u00f3n como usar el metodo DataFrame.drop().\n\nDocs: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html"}, {"metadata": {}, "cell_type": "code", "source": "df.tail()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "df2 = df.drop(columns=['reducido'], inplace=False)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "df2.tail()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Manejando variables Categoricas\n\nComo se menciono antes, los computadores no son buenos con las variables categoricas.\n\nMientras que nosotros entendemos bien las variables categoricas, es debido a un conocimiento previo quie el computador no tiene.\n\nLa mayoria de tecnicas de Machine Learning y modelso trabajan con un set limitado de datos (Tipicamente binario). \n\nLas redes neurales consumenda data y producen resultados en el rango de 0..1 t raramente van mas alla del alcance.\n\nEn resumen, la gran mayoria de algoritmos de machine learning aceptan data de entrada  (\"training data\") de donde los features son extraidos.\n\nBasado en estos features, un modelo matematico es creado, el cual es usado para hacer una predicci\u00f3n o decision sin ser programado explicitamente para esa tarea.\n\nDado un dataset con con 2 features, vamos a dejar que encoder encuentre los valores unicos por features y transforme la data a binario usando la tecnica one-hot encoding."}, {"metadata": {}, "cell_type": "code", "source": "# Columnas One-hot-encoding del dataset usando el metodo de Pandas ``get_dummies``  (demontraci\u00f3n)\ndf3 = pd.get_dummies(df2, columns=['ciclo'])\ndf3.tail()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<hr>"}, {"metadata": {}, "cell_type": "markdown", "source": "## Entrenando un clasificador basado  en un \u00c1rbol de Decisi\u00f3n"}, {"metadata": {}, "cell_type": "markdown", "source": "### Seleccionando FEATURES y definiendo la variable TARGET"}, {"metadata": {}, "cell_type": "code", "source": "df3.columns", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "features = df3[\n    [\n        'studentId', 'Calificacion_Promedio', 'Tareas_Puntuales',\n        'Tareas_No_Entregadas', 'Tareas_Retrasadas', 'Total_Tareas',\n        'ciclo_2017 - 03', 'ciclo_2017 - 04', 'ciclo_2017 - 05',\n        'ciclo_2017 - 06', 'ciclo_2017 - 07', 'ciclo_2017 - 08',\n        'ciclo_2018 - 01', 'ciclo_2018 - 02', 'ciclo_2018 - 03',\n        'ciclo_2018 - 04', 'ciclo_2018 - 05', 'ciclo_2018 - 06',\n        'ciclo_2018 - 07', 'ciclo_2018 - 08', 'ciclo_2019 - 01',\n        'ciclo_2019 - 02', 'ciclo_2019 - 03', 'ciclo_2019 - 04',\n        'ciclo_2019 - 05', 'ciclo_2019 - 06', 'ciclo_2019 - 07',\n        'ciclo_2019 - 08'\n    ]\n]\ntarget = df3['Graduado']  ## No cambie la variable target!", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Dividiendo nuestro dataset en set de Entrenamiento y Pruebas"}, {"metadata": {}, "cell_type": "code", "source": "from sklearn.model_selection import train_test_split\n\n\nX_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=133)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Entrenando un modelo ``DecisionTreeClassifier()``"}, {"metadata": {}, "cell_type": "code", "source": "# M\u00e9todo para creacion de modelos basados en arbol de desici\u00f3n\nfrom sklearn.tree import DecisionTreeClassifier\n\n\ndtc = DecisionTreeClassifier(max_depth=15).fit(X_train, y_train)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Haciendo predicciones del Sample Test"}, {"metadata": {}, "cell_type": "code", "source": "y_pred = dtc.predict(X_test)\nprint(y_pred)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Analice la calidad del modelo a trav\u00e9s de la matriz de confusi\u00f3n"}, {"metadata": {}, "cell_type": "code", "source": "!pip install seaborn --upgrade", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "from sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\ncf_matrix = confusion_matrix(y_test, y_pred)\ngroup_names = ['`Positivo` Correto', '`Negativo` Errado', 'Falso `Positivo`', '`Negativo` Correto']\ngroup_counts = ['{0:0.0f}'.format(value) for value in cf_matrix.flatten()]\ngroup_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten()/np.sum(cf_matrix)]\nlabels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in zip(group_names, group_counts, group_percentages)]\nlabels = np.asarray(labels).reshape(2,2)\naccuracy  = np.trace(cf_matrix) / float(np.sum(cf_matrix))\nprecision = cf_matrix[1,1] / sum(cf_matrix[:,1])\nrecall    = cf_matrix[1,1] / sum(cf_matrix[1,:])\nf1_score  = 2*precision*recall / (precision + recall)\nsns.heatmap(cf_matrix, annot=labels, fmt=\"\")\nstats_text = \"\\n\\nAccuracy={:0.3f}\\nPrecision={:0.3f}\\nRecall={:0.3f}\\nF1 Score={}\".format(accuracy, precision, recall, f1_score)\nplt.ylabel('True label')\nplt.xlabel('Predicted label' + stats_text)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<hr>"}, {"metadata": {}, "cell_type": "markdown", "source": "## Scoring de la data requerida para hacer la entrega de la soluci\u00f3n"}, {"metadata": {}, "cell_type": "markdown", "source": "Para el env\u00edo, necesita clasificar el siguiente dataset. Para hacer eso, usted necesita reproducir los mismos pasos de pre-procesamiento para que el dataset este en la misma estructura del que usted uso para construir su modelo. Despues de clasificar este dataframe, esperamos que usted entregue un archivo csv con las 2499 filar y una columna 'Graduado' con su predicci\u00f3n. **No cambie el orden del archivo a predecir ni borre filas**"}, {"metadata": {}, "cell_type": "code", "source": "!wget --no-check-certificate --content-disposition https://raw.githubusercontent.com/vanderlei-test/dataset2/master/for_submission/ToBePredicted.csv\ndf_to_be_predicted = pd.read_csv(r'ToBePredicted.csv')\ndf_to_be_predicted.tail()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Uniendo los dataset\ndf = pd.merge(\n    df_to_be_predicted, df_tabla_tareas, how='inner',\n    on=None, left_on=['studentId', 'ciclo'], right_on=['studentId', 'ciclo'],\n    left_index=False, right_index=False, sort=True,\n    suffixes=('_x', '_y'), copy=True, indicator=False,\n    validate=None\n)\n\n# Eliminando la columna 'reducido'\ndf2 = df.drop(columns=['reducido'], inplace=False)\n\n# Columnas One-hot-encoding del dataset usando el metodo de Pandas ``get_dummies``  (demontraci\u00f3n)\ndf3 = pd.get_dummies(df2, columns=['ciclo'])\ndf3.tail()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Observando los features declarados acontinuaci\u00f3n, sabemos que el dataset ha ser evaluado esta en el mismo formato usado para entrenar nuestro \u00e1rbol de decisi\u00f3n anteriormente.\n\n```features = df3[\n    [\n        'studentId', 'Calificacion_Promedio', 'Tareas_Puntuales',\n        'Tareas_No_Entregadas', 'Tareas_Retrasadas', 'Total_Tareas',\n        'ciclo_2017 - 03', 'ciclo_2017 - 04', 'ciclo_2017 - 05',\n        'ciclo_2017 - 06', 'ciclo_2017 - 07', 'ciclo_2017 - 08',\n        'ciclo_2018 - 01', 'ciclo_2018 - 02', 'ciclo_2018 - 03',\n        'ciclo_2018 - 04', 'ciclo_2018 - 05', 'ciclo_2018 - 06',\n        'ciclo_2018 - 07', 'ciclo_2018 - 08', 'ciclo_2019 - 01',\n        'ciclo_2019 - 02', 'ciclo_2019 - 03', 'ciclo_2019 - 04',\n        'ciclo_2019 - 05', 'ciclo_2019 - 06', 'ciclo_2019 - 07',\n        'ciclo_2019 - 08'\n    ]\n]\ntarget = df3['Graduado']  ## No cambie la variable target!```"}, {"metadata": {}, "cell_type": "code", "source": "y_pred = dtc.predict(df3[\n    [\n        'studentId', 'Calificacion_Promedio', 'Tareas_Puntuales',\n        'Tareas_No_Entregadas', 'Tareas_Retrasadas', 'Total_Tareas',\n        'ciclo_2017 - 03', 'ciclo_2017 - 04', 'ciclo_2017 - 05',\n        'ciclo_2017 - 06', 'ciclo_2017 - 07', 'ciclo_2017 - 08',\n        'ciclo_2018 - 01', 'ciclo_2018 - 02', 'ciclo_2018 - 03',\n        'ciclo_2018 - 04', 'ciclo_2018 - 05', 'ciclo_2018 - 06',\n        'ciclo_2018 - 07', 'ciclo_2018 - 08', 'ciclo_2019 - 01',\n        'ciclo_2019 - 02', 'ciclo_2019 - 03', 'ciclo_2019 - 04',\n        'ciclo_2019 - 05', 'ciclo_2019 - 06', 'ciclo_2019 - 07',\n        'ciclo_2019 - 08'\n    ]\n])\nprint(y_pred)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Guardando los resultados de la predicci\u00f3n en un archivo csv"}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "np.savetxt(\"results.csv\", y_pred, delimiter=\",\", fmt='%s')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "project.save_data(file_name=\"results.csv\", data=pd.read_csv(\"results.csv\", header=None).to_csv(header=[\"TARGET\"], index=False))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<hr>\n\n## \u00a1Felicitaciones!\n\nSi todo fue ejecutado sin errores, usted ya tiene un modelo basado en classificacion binaria y puede descargar sus resultados para subirlos como csv!\n\nPara enviar su soluci\u00f3n, ve a la p\u00e1gina:\n\n# https://anahuac.maratona.dev\n"}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.6", "language": "python"}, "language_info": {"name": "python", "version": "3.6.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}